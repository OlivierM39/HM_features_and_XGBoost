{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22038809-b546-47c6-8f45-3447e1b69345",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0b648e-fecf-417a-801d-c6bb04aca721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les classiques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from functools import reduce\n",
    "import json\n",
    "\n",
    "# Machine Learning\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost.callback import EarlyStopping\n",
    "import joblib\n",
    "\n",
    "# Métriques\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Graphiques\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25bc162-e7e1-40d1-a701-d3b8793da17e",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e953007-1d15-4155-8067-c2ed6d2c2acc",
   "metadata": {},
   "source": [
    "### Data preparation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26925ca-d8e3-440f-a132-00394a236837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 1. Data importation ===== #\n",
    "def data_importation(prisme_path, prisme_file, features_path, features_file, col_prisme, externe_col_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Imports the file containing the target variables and the file containing the features, and merges them into a single DataFrame.\n",
    "\n",
    "    Requires that the name of the column containing the target values starts with the target identifier\n",
    "    and that any suffixes are separated by an underscore \"_\" (e.g., \"P500_vel\", in which case the target name is \"P500\").\n",
    "    \"\"\"\n",
    "\n",
    "    # === i. Read === #\n",
    "    prisme_data  = pd.read_excel(os.path.join(prisme_path, prisme_file)) # Movement data\n",
    "    features_data = pd.read_excel(os.path.join(features_path, features_file)) # Features_data\n",
    "\n",
    "    # === ii. Formatting === #\n",
    "    prisme_name = col_prisme.split('_')[0]\n",
    "    prisme_data = prisme_data[['Date', col_prisme]].copy()\n",
    "    prisme_data.rename(columns={col_prisme: prisme_name}, inplace=True)\n",
    "\n",
    "    # === iii. Parsing & sorting === #\n",
    "    prisme_data['Date']  = pd.to_datetime(prisme_data['Date'], errors='coerce')\n",
    "    features_data['Date'] = pd.to_datetime(features_data['Date'], errors='coerce')\n",
    "    prisme_data  = prisme_data.sort_values('Date').reset_index(drop=True)\n",
    "    features_data = features_data.sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    # === iv. Build merged DataFrame === #\n",
    "    dfs = [prisme_data, features_data]\n",
    "    merged_df = reduce(lambda left, right: pd.merge(left, right, on='Date', how='inner'), dfs)\n",
    "    merged_df = merged_df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    # Info before cleaning\n",
    "    print(f\"[Avant dropna] n={len(merged_df)} | Date range: {merged_df['Date'].min()} -> {merged_df['Date'].max()}\")\n",
    "\n",
    "    # === v. Observable base (Date + target + external variables) === #\n",
    "    externes_ok = [c for c in externe_col_list if c in merged_df.columns]\n",
    "    base_df = merged_df[['Date', prisme_name] + externes_ok].copy()\n",
    "    \n",
    "    # === vi. Drop incomplete rows (target + features) === #\n",
    "    merged_df = merged_df.dropna().reset_index(drop=True)\n",
    "\n",
    "    # Info after cleaning\n",
    "    if len(merged_df):\n",
    "        print(f\"[Après  dropna] n={len(merged_df)} | Date range: {merged_df['Date'].min()} -> {merged_df['Date'].max()}\")\n",
    "    else:\n",
    "        print(\"[Après  dropna] n=0\")\n",
    "    \n",
    "    return base_df, merged_df, prisme_name\n",
    "\n",
    "# ===== 1Bis. Data importation of Séchilienne landslide ===== #\n",
    "def data_importation_Sech(prisme_data, features_data, col_prisme, externe_col_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Note: An adapted version of \"data_importation\" function, specific to Séchilienne data application.\n",
    "    \"\"\"\n",
    "    # === ii. Formatting === #\n",
    "    prisme_name = col_prisme.split('_')[0]\n",
    "    prisme_data = prisme_data[['Date', col_prisme]].copy()\n",
    "    prisme_data.rename(columns={col_prisme: prisme_name}, inplace=True)\n",
    "\n",
    "    # === iii. Parsing & sorting === #\n",
    "    prisme_data['Date']  = pd.to_datetime(prisme_data['Date'], errors='coerce')\n",
    "    features_data['Date'] = pd.to_datetime(features_data['Date'], errors='coerce')\n",
    "    prisme_data  = prisme_data.sort_values('Date').reset_index(drop=True)\n",
    "    features_data = features_data.sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    # === iv. Build merged DataFrame === #\n",
    "    dfs = [prisme_data, features_data]\n",
    "    merged_df = reduce(lambda left, right: pd.merge(left, right, on='Date', how='inner'), dfs)\n",
    "    merged_df = merged_df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    # Info before cleaning\n",
    "    print(f\"[Avant dropna] n={len(merged_df)} | Date range: {merged_df['Date'].min()} -> {merged_df['Date'].max()}\")\n",
    "\n",
    "    # === v. Observable base (Date + target + external variables) === #\n",
    "    externes_ok = [c for c in externe_col_list if c in merged_df.columns]\n",
    "    base_df = merged_df[['Date', prisme_name] + externes_ok].copy()\n",
    "    \n",
    "    # === vi. Drop incomplete rows (target + features) === #\n",
    "    merged_df = merged_df.dropna().reset_index(drop=True)\n",
    "\n",
    "    # Info after cleaning\n",
    "    if len(merged_df):\n",
    "        print(f\"[Après  dropna] n={len(merged_df)} | Date range: {merged_df['Date'].min()} -> {merged_df['Date'].max()}\")\n",
    "    else:\n",
    "        print(\"[Après  dropna] n=0\")\n",
    "    \n",
    "    return base_df, merged_df, prisme_name\n",
    "\n",
    "# ===== 2. Division de la série en 5 parts égales ===== #\n",
    "def data_splitting_preparation(merged_df, bloc_numbers):\n",
    "\n",
    "    \"\"\"\n",
    "    Splits the full time series into N equal parts.\n",
    "    \"\"\"\n",
    "\n",
    "    df = merged_df.copy()\n",
    "\n",
    "    bloc_size = len(df) // bloc_numbers\n",
    "    bloc_list = list(range(1, bloc_numbers + 1))\n",
    "\n",
    "    sub_dfs = {}\n",
    "    for i in range(bloc_numbers):\n",
    "        start_index = i*bloc_size\n",
    "        # Include the remaining rows in the last block\n",
    "        end_index = start_index + bloc_size if i != bloc_numbers - 1 else len(df)\n",
    "        sub_dfs[f'subdf_{i+1}'] = df.iloc[start_index : end_index]\n",
    "\n",
    "    # Check the length of each sub_df\n",
    "    for bloc in bloc_list:\n",
    "        subdf_name = f\"subdf_{bloc}\"\n",
    "        length = len(sub_dfs[subdf_name]) # (kept for debugging / verification)\n",
    "\n",
    "    # Build the list of sub_dfs\n",
    "    subdf_list = [sub_dfs[f'subdf_{i+1}'] for i in range(bloc_numbers)]\n",
    "\n",
    "    return subdf_list\n",
    "\n",
    "# ===== 3. Build train_df and test_df ===== #\n",
    "def train_test_splitting(subdf_list, test_df_index):\n",
    "\n",
    "    \"\"\"\n",
    "    Defines the test_df and the resulting train_df based on the index of the selected test block.\n",
    "    \"\"\"\n",
    "\n",
    "    test_df = subdf_list[test_df_index]\n",
    "\n",
    "    train_df_list = [subdf for i, subdf in enumerate(subdf_list) if i != test_df_index]\n",
    "    train_df = pd.concat(train_df_list, ignore_index=True)\n",
    "    train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
    "    train_df = train_df.sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "# ===== 4. Define targets (y) and feature matrices (X) for train_df and test_df ===== #\n",
    "def X_and_y_definition(train_df, test_df, prisme_name, externe_col_list): \n",
    "\n",
    "    \"\"\"\n",
    "    Splits X and y for both the train_df and the test_df.\n",
    "    \"\"\"\n",
    "\n",
    "    drop_cols = ['Date', prisme_name] + externe_col_list\n",
    "\n",
    "    X_train = train_df.drop(columns=drop_cols)\n",
    "    y_train = train_df[prisme_name]\n",
    "\n",
    "    X_test = test_df.drop(columns=drop_cols)\n",
    "    y_test = test_df[prisme_name]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# ===== 5. Pipeline step ===== #\n",
    "def pipeline_data_preparation(prisme_path, prisme_file, features_path, features_file, col_prisme, externe_col_list,\n",
    "                             bloc_numbers, test_df_index):\n",
    "\n",
    "    # == 1. Import == #\n",
    "    base_df, merged_df, prisme_name = data_importation(prisme_path, prisme_file, features_path, features_file, \n",
    "                                                       col_prisme, externe_col_list)\n",
    "\n",
    "    # == 2. Split the time series == #\n",
    "    subdf_list = data_splitting_preparation(merged_df, bloc_numbers)\n",
    "\n",
    "    # == 3. Define train_df and test_df == #\n",
    "    train_df, test_df = train_test_splitting(subdf_list, test_df_index)\n",
    "\n",
    "    # == 4. Define X and y for training and testing sets == #\n",
    "    X_train, y_train, X_test, y_test = X_and_y_definition(train_df, test_df, prisme_name, externe_col_list)\n",
    "\n",
    "    return base_df, train_df, test_df, X_train, y_train, X_test, y_test, subdf_list, prisme_name\n",
    "\n",
    "# ===== 5Bis. Pipeline step (Séchilienne case) ===== #\n",
    "def pipeline_data_preparation_Sech(prisme_data, features_data, col_prisme, externe_col_list, bloc_numbers, test_df_index):\n",
    "\n",
    "    # == 1. Import == #\n",
    "    base_df, merged_df, prisme_name = data_importation_Sech(prisme_data, features_data, col_prisme, externe_col_list)\n",
    "\n",
    "    # == 2. Split the time series == #\n",
    "    subdf_list = data_splitting_preparation(merged_df, bloc_numbers)\n",
    "\n",
    "    # == 3. Define train_df and test_df == #\n",
    "    train_df, test_df = train_test_splitting(subdf_list, test_df_index)\n",
    "\n",
    "    # == 4. Define X and y for training and testing sets == #\n",
    "    X_train, y_train, X_test, y_test = X_and_y_definition(train_df, test_df, prisme_name, externe_col_list)\n",
    "\n",
    "    return base_df, train_df, test_df, X_train, y_train, X_test, y_test, subdf_list, prisme_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a37947-4612-4555-b934-d51b76ae654e",
   "metadata": {},
   "source": [
    "### Machine learning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e61554-52a5-43cd-80f3-b458904e6451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 1. Metric computation ===== #\n",
    "def metrics_evaluation(y_obs, y_pred):\n",
    "\n",
    "    R2 = float(r2_score(y_obs, y_pred))\n",
    "    RMSE = float(np.sqrt(mean_squared_error(y_obs, y_pred)))\n",
    "    MAE = float(mean_absolute_error(y_obs, y_pred))\n",
    "\n",
    "    return {\"R2\": R2, \"RMSE\": RMSE, \"MAE\": MAE, \"n\": int(len(y_obs))}\n",
    "\n",
    "# ===== 2. XGBoost model ===== #\n",
    "def XGB_model(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # == 1. Model training == #\n",
    "    model = XGBRegressor(n_estimators=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # == 2. Retrieve hyperparameter values == #\n",
    "    hyperparameter_values = model.get_params()\n",
    "    params_df = pd.DataFrame(hyperparameter_values.items(), columns=[\"Hyperparameter\", \"Value\"])\n",
    "\n",
    "    # == 3. Predictions on train and test sets == #\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    # == 4. Compute metrics == #\n",
    "    train_metrics = metrics_evaluation(y_train, y_pred_train)\n",
    "    test_metrics = metrics_evaluation(y_test, y_pred_test)\n",
    "\n",
    "    metrics_df = pd.DataFrame.from_dict({\"train\": train_metrics, \"test\": test_metrics}, \n",
    "                                        orient=\"index\").loc[[\"train\", \"test\"]].round({\"R2\": 3, \"RMSE\": 3, \"MAE\": 3})\n",
    "\n",
    "    # == 5. Feature importance == #\n",
    "    # Feature importance based on \"gain\"\n",
    "    importances = model.feature_importances_\n",
    "    array_importance = np.array([X_test.columns, importances])\n",
    "    \n",
    "    # Build a DataFrame of features ranked by importance\n",
    "    FI_df = pd.DataFrame(np.transpose(array_importance), columns=['Features', 'Importances'])\n",
    "    FI_df = FI_df.sort_values(by='Importances', ascending=False)\n",
    "    FI_top = FI_df.head(20)\n",
    "\n",
    "    # == 6. SHAP values == #\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X_test)\n",
    "\n",
    "    return y_pred_train, y_pred_test, params_df, metrics_df, FI_df, FI_top, shap_values, model\n",
    "\n",
    "# ===== 3. Create the output DataFrame for predicted values ===== #\n",
    "def output_df_creation(base_df, train_df, test_df, y_test, y_pred_train, y_pred_test):\n",
    "\n",
    "    # Dates and time span\n",
    "    date_start = pd.to_datetime(base_df[\"Date\"]).min()\n",
    "    date_end   = pd.to_datetime(base_df[\"Date\"]).max()\n",
    "    full_dates = pd.DataFrame({\"Date\": pd.date_range(start=date_start, end=date_end, freq=\"D\")})\n",
    "\n",
    "    # Train and test dates\n",
    "    train_dates = train_df[\"Date\"].values \n",
    "    test_dates = test_df[\"Date\"].values\n",
    "\n",
    "    pred_train_df = pd.DataFrame({\n",
    "        'Date': train_dates, \n",
    "        'vel_train_pred': pd.Series(y_pred_train).values\n",
    "    })\n",
    "\n",
    "    pred_test_df = pd.DataFrame({\n",
    "        'Date': test_dates, \n",
    "        'vel_test': pd.Series(y_test).values, \n",
    "        'vel_test_pred': pd.Series(y_pred_test).values\n",
    "    })\n",
    "\n",
    "    dfs = [base_df, pred_train_df, pred_test_df]\n",
    "    merged = reduce(lambda left, right: pd.merge(left, right, on='Date', how='outer'), dfs)\n",
    "    out_df = full_dates.merge(merged, on=\"Date\", how=\"left\").sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "    return out_df\n",
    "\n",
    "# ===== 4. Pipeline step ===== #\n",
    "def pipeline_XGBoost_application(base_df, train_df, test_df, X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # == 1. XGBoost == #\n",
    "    y_pred_train, y_pred_test, params_df, metrics_df, FI_df, FI_top, shap_values, model = XGB_model(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # == 2. Output DataFrame == #\n",
    "    out_df = output_df_creation(base_df, train_df, test_df, y_test, y_pred_train, y_pred_test)\n",
    "\n",
    "    return out_df, params_df, metrics_df, FI_df, FI_top, shap_values, model\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad4e86-f880-410d-b23d-beeaaeb6370b",
   "metadata": {},
   "source": [
    "### Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa28c071-6fcb-4e86-813c-50ad0d837cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 0. Extract metric values for display ===== #\n",
    "def metrics_box_text(metrics_df, split=\"test\", keys=(\"RMSE\",\"MAE\",\"R2\"), fmt=(\"{:.3f}\",\"{:.3f}\",\"{:.3f}\")):\n",
    "    row = metrics_df.loc[split]\n",
    "    lines = []\n",
    "    for k, f in zip(keys, fmt):\n",
    "        if k in metrics_df.columns:\n",
    "            val = row[k]\n",
    "            lines.append(f\"{k}: {f.format(val)}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ===== 1. Time-series plot ===== #\n",
    "def graphique_1(out_df, prisme_name, test_df_index, metrics_df, show_figure):\n",
    "\n",
    "    \"\"\"\n",
    "    Displays:\n",
    "      - Top subplot   : prisme_name (observed, full time series) + vel_pred (on train and test)\n",
    "      - Bottom subplot: External time series (ER + GWL)\n",
    "    \"\"\"\n",
    "    \n",
    "    df = out_df.copy()\n",
    "    \n",
    "    # === Figure === #\n",
    "    fig1, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,7))\n",
    "\n",
    "    # Figure 1 \n",
    "    ax1.plot(df['Date'], df[prisme_name], label=f'{prisme_name} observed', color='grey', alpha=0.8, lw=1.3)\n",
    "    ax1.plot(df['Date'], df['vel_train_pred'], label=f'{prisme_name} simulated on train', color='maroon', lw=1)\n",
    "    ax1.plot(df['Date'], df['vel_test_pred'], label=f'{prisme_name} simulated on test', color='blue', lw=1)\n",
    "\n",
    "    ax1.set_ylabel(\"Velocity (cm/day)\")\n",
    "    ax1.set_title(f\"Observed vs simulated velocity with test_set n°{test_df_index+1}\")\n",
    "    ax1.grid(True, alpha=0.2)\n",
    "    ax1.spines['top'].set_visible(False), ax1.spines['right'].set_visible(False)\n",
    "    ax1.legend(loc=\"best\")\n",
    "\n",
    "    txt = metrics_box_text(metrics_df, split=\"test\", keys=(\"RMSE\",\"MAE\",\"R2\"), fmt=(\"{:.3f}\",\"{:.3f}\",\"{:.3f}\"))\n",
    "    ax1.text(0.01, 0.98, txt,\n",
    "        transform=ax1.transAxes, va=\"top\", ha=\"left\",\n",
    "        fontsize=10,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", alpha=0.8))\n",
    "\n",
    "    # Figure 2\n",
    "    ax2.plot(df['Date'], df['GWL'], label='Groundwater level', color='navy')\n",
    "    ax2.set_ylabel(\"Groundwater level (m)\")\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "\n",
    "    ax2b = ax2.twinx()\n",
    "    ax2b.bar(df['Date'], df['ER'], label='Effective rainfall', color='deepskyblue', width=0.7, alpha=0.7)\n",
    "    ax2b.invert_yaxis()\n",
    "    ax2b.set_ylabel(\"Effective rainfall (mm)\")\n",
    "    ax2b.spines['top'].set_visible(False)\n",
    "\n",
    "    ax2.set_title(\"External time series (ER + GWL)\")\n",
    "    ax2.grid(True, alpha=0.2)\n",
    "\n",
    "    fig1.tight_layout()\n",
    "\n",
    "    if show_figure:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig1)\n",
    "\n",
    "    return fig1\n",
    "\n",
    "# ===== 1BIS.Time-series plot (Séchilienne case) ===== #\n",
    "def graphique_1bis(out_df, prisme_name, test_df_index, metrics_df, show_figure):\n",
    "\n",
    "    \"\"\"\n",
    "    Displays:\n",
    "      - Top subplot   : prisme_name (observed, full time series) + vel_pred (on train and test)\n",
    "      - Bottom subplot: External time series (ER + GWL)\n",
    "    \"\"\"\n",
    "    \n",
    "    df = out_df.copy()\n",
    "    \n",
    "    # === Figure === #\n",
    "    fig1, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,7))\n",
    "\n",
    "    # Figure 1 \n",
    "    ax1.plot(df['Date'], df[prisme_name], label=f'{prisme_name} observed', color='grey', alpha=0.8, lw=1.3)\n",
    "    ax1.plot(df['Date'], df['vel_train_pred'], label=f'{prisme_name} simulated on train', color='maroon', lw=1)\n",
    "    ax1.plot(df['Date'], df['vel_test_pred'], label=f'{prisme_name} simulated on test', color='blue', lw=1)\n",
    "\n",
    "    ax1.set_ylabel(\"Velocity (cm/day)\")\n",
    "    ax1.set_title(f\"Observed vs simulated velocity with test_set n°{test_df_index+1}\")\n",
    "    ax1.grid(True, alpha=0.2)\n",
    "    ax1.spines['top'].set_visible(False), ax1.spines['right'].set_visible(False)\n",
    "    ax1.legend(loc=\"best\")\n",
    "\n",
    "    txt = metrics_box_text(metrics_df, split=\"test\", keys=(\"RMSE\",\"MAE\",\"R2\"), fmt=(\"{:.3f}\",\"{:.3f}\",\"{:.3f}\"))\n",
    "    ax1.text(0.01, 0.98, txt,\n",
    "        transform=ax1.transAxes, va=\"top\", ha=\"left\",\n",
    "        fontsize=10,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", alpha=0.8))\n",
    "\n",
    "    # Figure 2\n",
    "    ax2.plot(df['Date'], df['WLI'], label='Groundwater level', color='dodgerblue')\n",
    "    ax2.plot(df['Date'], df['WLM'], label='Groundwater level', color='navy')\n",
    "    ax2.set_ylabel(\"Groundwater level (m)\")\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "\n",
    "    ax2b = ax2.twinx()\n",
    "    ax2b.bar(df['Date'], df['ER'], label='Effective rainfall', color='deepskyblue', width=0.7, alpha=0.7)\n",
    "    ax2b.invert_yaxis()\n",
    "    ax2b.set_ylabel(\"Effective rainfall (mm)\")\n",
    "    ax2b.spines['top'].set_visible(False)\n",
    "\n",
    "    ax2.set_title(\"External time series (ER + GWL)\")\n",
    "    ax2.grid(True, alpha=0.2)\n",
    "\n",
    "    fig1.tight_layout()\n",
    "\n",
    "    if show_figure:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig1)\n",
    "\n",
    "    return fig1\n",
    "\n",
    "# ===== 2. Features importance ===== #\n",
    "def graphique_2(FI_top, prisme_name, test_df_index, show_figure):\n",
    "\n",
    "    \"\"\"\n",
    "    Feature importance ranking (displays only a limited number).\n",
    "    \"\"\"\n",
    "\n",
    "    # === Prepare colour assignment for bars === #\n",
    "    prefix_color_map = {\"GWL\": \"rebeccapurple\",\n",
    "                        \"WLI\": \"darkorchid\",\n",
    "                        \"WLM\": \"rebeccapurple\",\n",
    "                        \"R\":   \"steelblue\",\n",
    "                        \"ER\":  \"deepskyblue\",\n",
    "                       }\n",
    "\n",
    "    # Colour assignment function\n",
    "    def assign_color(feature):\n",
    "        for prefix, color in prefix_color_map.items():\n",
    "            if feature.startswith(prefix):\n",
    "                return color\n",
    "        return \"gray\"  # couleur par défaut\n",
    "\n",
    "    # Preparation\n",
    "    df = FI_top.copy()\n",
    "    colors = df[\"Features\"].map(assign_color)\n",
    "    \n",
    "    # === Figure === #\n",
    "    fig2, ax = plt.subplots(figsize=(12,4))\n",
    "    bars = ax.bar(df[\"Features\"], df[\"Importances\"], color=colors, edgecolor=\"black\")\n",
    "    ax.set_ylabel(\"Importance\")\n",
    "    ax.set_xticks(range(len(df[\"Features\"])))\n",
    "    ax.set_xticklabels(df[\"Features\"], rotation=45, ha=\"right\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.grid(axis=\"y\", alpha=0.2)\n",
    "\n",
    "    ax.set_title(f\"Top Feature Importance for {prisme_name} | Test bloc n°{test_df_index+1}\")\n",
    "\n",
    "    fig2.tight_layout()\n",
    "    \n",
    "    if show_figure:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig2)\n",
    "\n",
    "    return fig2\n",
    "\n",
    "# ===== 3. SHAP values ===== #\n",
    "def graphique_3(shap_values, X_test, prisme_name, test_df_index, show_figure):\n",
    "\n",
    "    \"\"\"\n",
    "    SHAP values.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12,4))\n",
    "\n",
    "    shap.summary_plot(shap_values, X_test, show=False)\n",
    "    fig3 = plt.gcf()\n",
    "    fig3.suptitle(f\"SHAP summary for {prisme_name} | Test bloc n°{test_df_index+1}\")\n",
    "    fig3.tight_layout()\n",
    "\n",
    "    if show_figure:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig3)    \n",
    "\n",
    "    return fig3\n",
    "\n",
    "# ===== 4. Pipeline step ===== #\n",
    "def pipeline_graphique(out_df, FI_top, shap_values, X_test, prisme_name, test_df_index, metrics_df, show_figure):\n",
    "    \n",
    "    fig1 = graphique_1(out_df, prisme_name, test_df_index, metrics_df, show_figure)\n",
    "    fig2 = graphique_2(FI_top, prisme_name, test_df_index, show_figure)\n",
    "    fig3 = graphique_3(shap_values, X_test, prisme_name, test_df_index, show_figure)\n",
    "\n",
    "    return fig1, fig2, fig3\n",
    "\n",
    "# ===== 4BIS. Pipeline step for Séchilienne ===== #\n",
    "def pipeline_graphique_Sech(out_df, FI_top, shap_values, X_test, prisme_name, test_df_index, metrics_df, show_figure):\n",
    "    \n",
    "    fig1 = graphique_1bis(out_df, prisme_name, test_df_index, metrics_df, show_figure)\n",
    "    fig2 = graphique_2(FI_top, prisme_name, test_df_index, show_figure)\n",
    "    fig3 = graphique_3(shap_values, X_test, prisme_name, test_df_index, show_figure)\n",
    "\n",
    "    return fig1, fig2, fig3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31685d8d-960b-4d0f-8459-e32b13675fef",
   "metadata": {},
   "source": [
    "### Saving functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bfc0d2-d5a1-4637-a37b-b8dcdb11b6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save(out_df, params_df, metrics_df, FI_df, FI_top, fig1, fig2, fig3, model, prisme_name, test_df_index, output_path):\n",
    "\n",
    "    # == 1. Create the output directory == #\n",
    "    out_dir = os.path.join(output_path, fr'2_XGBoost_results\\{prisme_name}\\Bloc_{test_df_index+1}')\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # == 2. Save outputs == #\n",
    "    # Save Excel files\n",
    "    out_df.to_excel(os.path.join(out_dir, 'Results_time_series.xlsx'), index=False)\n",
    "    params_df.to_excel(os.path.join(out_dir, 'Results_hyperparameters.xlsx'), index=False)\n",
    "    metrics_df.to_excel(os.path.join(out_dir, 'Results_metrics.xlsx'), index=False)\n",
    "    FI_df.to_excel(os.path.join(out_dir, 'Results_features_importance.xlsx'), index=False)\n",
    "    FI_top.to_excel(os.path.join(out_dir, 'Results_top_features_importance.xlsx'), index=False) \n",
    "    print(f\".XLSX files saved in {out_dir}\")\n",
    "\n",
    "    # Save figures\n",
    "    fig1.savefig(os.path.join(out_dir, \"Results_time_series.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "    fig2.savefig(os.path.join(out_dir, \"Results_features_importance.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "    fig3.savefig(os.path.join(out_dir, \"Results_shap_values.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\".PNG files saved in {out_dir}\")\n",
    "\n",
    "    # Save the trained model\n",
    "    joblib.dump(model, os.path.join(out_dir, \"XGB_trained_model.joblib\"))\n",
    "    print(f\".JOBLIB file saved in {out_dir}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2774b44-ef7f-4bb4-95f4-141af724876c",
   "metadata": {},
   "source": [
    "## End-to-end pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656e8450-61c1-41f1-b9fb-6523c35d8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_totale(prisme_path, prisme_file, features_path, features_file, col_prisme, externe_col_list,\n",
    "                    bloc_numbers, test_df_index, output_path, show_figure):\n",
    "\n",
    "    # ===== 1. Data reading and preparation ===== #\n",
    "    base_df, train_df, test_df, X_train, y_train, X_test, y_test, subdf_list, prisme_name = pipeline_data_preparation(prisme_path, prisme_file, \n",
    "                                                                                                             features_path, features_file,\n",
    "                                                                                                             col_prisme, externe_col_list,\n",
    "                                                                                                             bloc_numbers, test_df_index)\n",
    "\n",
    "    # ===== 2. XGBoost model ===== #\n",
    "    out_df, params_df, metrics_df, FI_df, FI_top, shap_values, model = pipeline_XGBoost_application(base_df, train_df, test_df, \n",
    "                                                                                                    X_train, y_train, X_test, y_test)\n",
    "\n",
    "    \n",
    "    # ===== 3. Figures ===== #\n",
    "    fig1, fig2, fig3 = pipeline_graphique(out_df, FI_top, shap_values, X_test, prisme_name, test_df_index, metrics_df, show_figure)\n",
    "\n",
    "    # ===== 4. Saving ===== #\n",
    "    Save(out_df, params_df, metrics_df, FI_df, FI_top, fig1, fig2, fig3, model, prisme_name, test_df_index, output_path)\n",
    "\n",
    "    return out_df, metrics_df, FI_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b934f755-eebe-4754-a86f-a072086f0461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_totale_Sech(prisme_data, features_data, col_prisme, externe_col_list,\n",
    "                    bloc_numbers, test_df_index, output_path, show_figure):\n",
    "\n",
    "    # ===== 1. Data reading and preparation ===== #\n",
    "    base_df, train_df, test_df, X_train, y_train, X_test, y_test, subdf_list, prisme_name = pipeline_data_preparation_Sech(prisme_data, \n",
    "                                                                                                       features_data, col_prisme, \n",
    "                                                                                                       externe_col_list, bloc_numbers, \n",
    "                                                                                                       test_df_index)\n",
    "\n",
    "    # ===== 2. XGBoost model ===== #\n",
    "    out_df, params_df, metrics_df, FI_df, FI_top, shap_values, model = pipeline_XGBoost_application(base_df, train_df, test_df, X_train, \n",
    "                                                                                                    y_train, X_test, y_test)\n",
    "\n",
    "    \n",
    "    # ===== 3. Figures ===== #\n",
    "    fig1, fig2, fig3 = pipeline_graphique_Sech(out_df, FI_top, shap_values, X_test, prisme_name, test_df_index, metrics_df, show_figure)\n",
    "\n",
    "    # ===== 4. Saving ===== #\n",
    "    Save(out_df, params_df, metrics_df, FI_df, FI_top, fig1, fig2, fig3, model, prisme_name, test_df_index, output_path)\n",
    "\n",
    "    return out_df, metrics_df, FI_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f329b9-4d8b-44ab-8546-e17540f5cc94",
   "metadata": {},
   "source": [
    "# Example usage\n",
    "\n",
    "This notebook is designed to be run from the **root of the GitHub repository**.\n",
    "\n",
    "Expected folder structure:\n",
    "\n",
    "- `data/<SITE>/0_Input_dataset/` : movement/target time series (Excel)\n",
    "- `data/<SITE>/1_Features_ER_R_GWL/` : pre-computed hydrometeorological features (Excel)\n",
    "- `outputs/<SITE>/` : results will be written here (created automatically)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f089582-27de-4c59-bebc-4841f245c5dd",
   "metadata": {},
   "source": [
    "## Viella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065eefb8-ba8c-4ce1-be1b-fc695120b846",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "SITE = 'Viella'\n",
    "\n",
    "# == Repository-relative paths == #\n",
    "repo_root = Path('.').resolve()\n",
    "prisme_path = repo_root / 'data' / SITE / '0_Input_dataset'\n",
    "features_path = repo_root / 'data' / SITE / '1_Features_ER_R_GWL'\n",
    "output_path = repo_root / 'outputs' / SITE\n",
    "\n",
    "prisme_file = 'vel_traité_all_synthese.xlsx'\n",
    "features_file = 'Features_HM.xlsx'\n",
    "\n",
    "col_list = ['BAV-01', 'CAH-I8', 'MID-03', 'VIL-11', 'VIL-14', 'VIL-16']\n",
    "externe_col_list = ['R', 'ER', 'GWL']\n",
    "\n",
    "# == Train-test split settings == #\n",
    "bloc_numbers = 5\n",
    "test_df_index_list = [0, 1, 2, 3, 4]\n",
    "\n",
    "# == Run == #\n",
    "show_figure = False\n",
    "for col_name in col_list:\n",
    "    col_prisme = f\"{col_name}_vel_processed\"\n",
    "    for test_df_index in test_df_index_list:\n",
    "        out_df, metrics_df, FI_df = pipeline_totale(\n",
    "            str(prisme_path), prisme_file,\n",
    "            str(features_path), features_file,\n",
    "            col_prisme, externe_col_list,\n",
    "            bloc_numbers, test_df_index,\n",
    "            str(output_path), show_figure\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02ae4df-a0eb-4d92-bec9-5a327aee988f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Villerville"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51c2937-58ca-4832-b5b9-835c251497cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "SITE = 'Villerville'\n",
    "\n",
    "# == Repository-relative paths == #\n",
    "repo_root = Path('.').resolve()\n",
    "prisme_path = repo_root / 'data' / SITE / '0_Input_dataset'\n",
    "features_path = repo_root / 'data' / SITE / '1_Features_ER_R_GWL'\n",
    "output_path = repo_root / 'outputs' / SITE\n",
    "\n",
    "prisme_file = 'vel_traité_all_synthese.xlsx'\n",
    "features_file = 'Features_HM.xlsx'\n",
    "\n",
    "col_list = ['VIL-PL35']\n",
    "externe_col_list = ['R', 'ER', 'GWL']\n",
    "\n",
    "# == Train-test split settings == #\n",
    "bloc_numbers = 5\n",
    "test_df_index_list = [0, 1, 2, 3, 4]\n",
    "\n",
    "# == Run == #\n",
    "show_figure = False\n",
    "for col_name in col_list:\n",
    "    col_prisme = f\"{col_name}_vel_processed\"\n",
    "    for test_df_index in test_df_index_list:\n",
    "        out_df, metrics_df, FI_df = pipeline_totale(\n",
    "            str(prisme_path), prisme_file,\n",
    "            str(features_path), features_file,\n",
    "            col_prisme, externe_col_list,\n",
    "            bloc_numbers, test_df_index,\n",
    "            str(output_path), show_figure\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663b8bc3-e589-46ec-8045-2c0272f5df88",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Séchilienne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210960b3-0ada-488d-ac57-3936bc5731ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "SITE = 'Séchilienne'\n",
    "repo_root = Path('.').resolve()\n",
    "prisme_path = repo_root / 'data' / SITE / '0_Input_dataset'\n",
    "features_path = repo_root / 'data' / SITE / '1_Features_ER_R_GWL'\n",
    "output_path = repo_root / 'outputs' / SITE\n",
    "\n",
    "prisme_file = 'vel_traité_decomposé_all_synthese.xlsx'\n",
    "features_file = 'Features_HM.xlsx'\n",
    "\n",
    "# === Read once (faster when looping over targets) ===\n",
    "prisme_data = pd.read_excel(os.path.join(prisme_path, prisme_file))\n",
    "features_data = pd.read_excel(os.path.join(features_path, features_file))\n",
    "\n",
    "col_list = ['E-A13', 'E-A16', 'E-C2', 'P1103', 'P1300']\n",
    "externe_col_list = ['R', 'ER', 'WLI', 'WLM']\n",
    "\n",
    "bloc_numbers = 5\n",
    "test_df_index_list = [0, 1, 2, 3, 4]\n",
    "\n",
    "show_figure = False\n",
    "for col_name in col_list:\n",
    "    col_prisme = f'{col_name}_vel_processed_decomposed'\n",
    "    for test_df_index in test_df_index_list:\n",
    "        out_df, metrics_df, FI_df = pipeline_totale_Sech(\n",
    "            prisme_data, features_data,\n",
    "            col_prisme, externe_col_list,\n",
    "            bloc_numbers, test_df_index,\n",
    "            str(output_path), show_figure\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5501929d-0da8-4e22-b86e-d91bee28cd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Add additional runs or parameter sweeps here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GWL-LSTM)",
   "language": "python",
   "name": "gwl-lstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
