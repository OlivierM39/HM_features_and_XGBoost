{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52e05435-6205-43fa-8981-26acbf5475eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa425a7-e339-4c08-b8bc-71d53c9c9e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from functools import reduce\n",
    "from typing import List, Dict, Iterable, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a96bc-a1b7-47fa-81f6-080e709db263",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a60e90-5377-4935-8d64-a7589cfea5f5",
   "metadata": {},
   "source": [
    "### Helper functions for data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4ba32a-d966-48e6-8918-383208aba6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 1. Read and format Excel files ===== #\n",
    "def read_excel_file(input_path:str, input_file:str, usecols:Optional[List[str]] = None) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    Reads an Excel file.\n",
    "\n",
    "    Optional: usecols allows you to specify which columns to keep from the Excel file.\n",
    "    It accepts up to 3 columns.\n",
    "    These columns must correspond to Rainfall, Effective rainfall, and Groundwater level values.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_excel(os.path.join(input_path, input_file))\n",
    "    \n",
    "    if 'Date' in df.columns:\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "        df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    if usecols:\n",
    "        keep = ['Date'] + [c for c in usecols if c in df.columns]\n",
    "        df = df[keep]\n",
    "\n",
    "    return df\n",
    "\n",
    "# ===== 2. Data import ===== #\n",
    "def data_importation(input_path, input_file, input_usecols):\n",
    "\n",
    "    \"\"\"\n",
    "    Uses the read_excel_file function.\n",
    "    Returns the DataFrame corresponding to the imported Excel file, while renaming the columns defined in usecols.\n",
    "\n",
    "    Output DataFrame -> Values of the predictive external time series.\n",
    "    \"\"\"\n",
    "\n",
    "    hm_df = read_excel_file(input_path, input_file, input_usecols)\n",
    "\n",
    "    # Rename historical columns to standard names\n",
    "    rename_map = {old: new for old, new in zip(input_usecols, ['R', 'ER', 'GWL'][:len(input_usecols)])}\n",
    "    hm_df = hm_df.rename(columns=rename_map)\n",
    "\n",
    "    return hm_df\n",
    "\n",
    "# ===== 2BIS. Data import for Séchilienne ===== #\n",
    "def data_importation_Sech(input_path, input_file, input_usecols):\n",
    "\n",
    "    hm_df = read_excel_file(input_path, input_file, input_usecols)\n",
    "\n",
    "    # Rename historical columns to standard names\n",
    "    rename_map = {old: new for old, new in zip(input_usecols, ['R', 'ER', 'WLI', 'WLM'][:len(input_usecols)])}\n",
    "    hm_df = hm_df.rename(columns=rename_map)\n",
    "\n",
    "    return hm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f11697-ea0b-4c9e-9549-d19debcff934",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Helper functions to compute rainfall (R) and effective rainfall (ER) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7410809-0db9-406b-8f62-37fc94ee9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FEATURES : Rolling features ===== #\n",
    "def _rolling_features(df, cols, T_periods):\n",
    "\n",
    "    out = {}\n",
    "    for col in cols:\n",
    "        s = df[col]\n",
    "        for T in T_periods:\n",
    "            out[f'{col}_{T}'] = s.rolling(window=T).sum()\n",
    "            out[f'{col}_max{T}'] = s.rolling(window=T).max()\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ===== FEATURES : Saturation indices ===== #\n",
    "def _staturation_indices(df, cols, short_terms=(3, 5, 10, 20, 30, 40, 60), long_terms=(30, 60, 90),\n",
    "                        rain_threshold=3.0, eps=1e-6):\n",
    "\n",
    "    out = {}\n",
    "    for col in cols:\n",
    "        filtered = df[col].where(df[col] >= rain_threshold, 0.0)\n",
    "        \n",
    "        for sT in short_terms:\n",
    "            short_sum = filtered.rolling(window=sT, min_periods=1).sum()\n",
    "            for lT in long_terms:\n",
    "                long_sum = filtered.shift(sT).rolling(window=lT, min_periods=1).sum()\n",
    "\n",
    "                ratio = short_sum / long_sum.mask(long_sum.abs() < eps, 1.0)\n",
    "                out[f'{col}_sat{sT}/{lT}'] = ratio\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ===== FEATURES : Anomaly indices ===== #\n",
    "def _anomaly_indices(df, cols, T_periods):\n",
    "\n",
    "    out = {}\n",
    "    for col in cols:\n",
    "        clim_mean = df.loc[df[col] > 0, col].mean()\n",
    "\n",
    "        for T in T_periods:\n",
    "            out[f'{col}_anomaly{T}'] = df[col].rolling(window=T).mean() / clim_mean\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ===== Event ID definition ===== #\n",
    "def _compute_event_ids(series, dry_tolerance=2):\n",
    "\n",
    "    \"\"\"\n",
    "    Event series with a tolerance of 'dry_tolerance' internal dry days.\n",
    "    Returns a np.ndarray of IDs (0 = no event).\n",
    "    \"\"\"\n",
    "\n",
    "    ids = np.zeros(len(series), dtype=int)\n",
    "    ev_id = 0\n",
    "    dry = 0\n",
    "\n",
    "    for i, v in enumerate(series.values):\n",
    "        if v > 0:\n",
    "            if ev_id == 0:\n",
    "                ev_id = (ids.max() + 1) if ids.max() > 0 else 1\n",
    "            dry = 0\n",
    "            ids[i] = ev_id\n",
    "        else:\n",
    "            if ev_id == 0:\n",
    "                ids[i] = ev_id\n",
    "            else:\n",
    "                dry += 1\n",
    "                if dry <= dry_tolerance:\n",
    "                    ids[i] = ev_id\n",
    "                else:\n",
    "                    ev_id = 0\n",
    "                    dry = 0\n",
    "                    ids[i] = 0\n",
    "\n",
    "    return ids\n",
    "\n",
    "\n",
    "# ===== Event strength calculation ===== #\n",
    "def _event_strength_for_col(df, col, dry_tolerance=2):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes, for a given column, an event-strength index:\n",
    "    (event cumulative sum) / (mean cumulative sum of events with the same duration).\n",
    "    \"\"\"\n",
    "    \n",
    "    s = df[col]\n",
    "    ev_ids = _compute_event_ids(s, dry_tolerance=dry_tolerance)\n",
    "\n",
    "    # Cumulative sum & duration per event\n",
    "    ev_df = pd.DataFrame({'id': ev_ids, 'val': s.values})\n",
    "    ev_df = ev_df[ev_df['id'] != 0]\n",
    "    if ev_df.empty:\n",
    "        return pd.Series(np.zeros(len(df)), index=df.index)\n",
    "\n",
    "    grp = ev_df.groupby('id')['val']\n",
    "    cumuls = grp.sum()\n",
    "    durations = ev_df.groupby('id').size()\n",
    "\n",
    "    # Mean cumulative sums by duration\n",
    "    tmp = pd.DataFrame({'Duration': durations, 'Cumul': cumuls})\n",
    "    mean_cumul_by_duration = tmp.groupby('Duration')['Cumul'].mean().to_dict()\n",
    "\n",
    "    # Map back to the original index\n",
    "    ev_cumul = pd.Series(cumuls, name='cumul')\n",
    "    ev_dur = pd.Series(durations, name='dur')\n",
    "\n",
    "    ev_map_cumul = pd.Series(0.0, index=np.unique(ev_ids[ev_ids != 0]))\n",
    "    ev_map_dur = pd.Series(0, index=np.unique(ev_ids[ev_ids != 0]))\n",
    "    ev_map_cumul.loc[ev_cumul.index] = ev_cumul.values\n",
    "    ev_map_dur.loc[ev_dur.index] = ev_dur.values\n",
    "\n",
    "    # Build the index series\n",
    "    idx_vals = []\n",
    "    for eid in ev_ids:\n",
    "        if eid == 0:\n",
    "            idx_vals.append(np.nan)\n",
    "        else:\n",
    "            dur = int(ev_map_dur.loc[eid])\n",
    "            cumul = float(ev_map_cumul.loc[eid])\n",
    "            denom = mean_cumul_by_duration.get(dur, np.nan)\n",
    "            idx_vals.append(cumul / denom if denom not in (0, np.nan) else np.nan)\n",
    "\n",
    "    return pd.Series(idx_vals, index=df.index)\n",
    "\n",
    "\n",
    "# ===== FEATURES : Event strength ===== #\n",
    "def _event_strength_pipeline(df, cols, dry_tolerance=2):\n",
    "\n",
    "    out = {}\n",
    "    for col in cols:\n",
    "        out[f'{col}_event_strength'] = _event_strength_for_col(df, col, dry_tolerance=dry_tolerance)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c80186d-964c-483f-ad3a-f442befc2008",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Functions to compute R and ER features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1d5610-e916-4233-9364-c1d9438ee341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Compute R/ER features =====\n",
    "def rainfall_and_effective_rainfall_features_calcul(df, meteo_col_list, T_periods=(1, 2, 5, 10, 20, 30, 60, 90)):\n",
    "\n",
    "    base_df = df[['Date'] + meteo_col_list].copy()\n",
    "    \n",
    "    # == 1. Rolling sums & max == #\n",
    "    roll_dict = _rolling_features(df, meteo_col_list, T_periods)\n",
    "\n",
    "    # == 2. Local saturation index == #\n",
    "    sat_dict = _staturation_indices(df, meteo_col_list, short_terms=(3,5,10,20,30,40,60), long_terms=(30,60,90), rain_threshold=3.0)\n",
    "\n",
    "    # == 3. Anomalies (recent mean / mean over rainy days) == #\n",
    "    anom_dict = _anomaly_indices(df, meteo_col_list, T_periods)\n",
    "\n",
    "    # == 4. Event-based indices == #\n",
    "    evt_dict = _event_strength_pipeline(df, meteo_col_list, dry_tolerance=2.0)\n",
    "\n",
    "    # == 5. Concatenate features == #\n",
    "    feats = pd.concat([base_df, pd.DataFrame(roll_dict), pd.DataFrame(sat_dict), \n",
    "                       pd.DataFrame(anom_dict), pd.DataFrame(evt_dict)], axis=1)\n",
    "\n",
    "    # == 6. Drop unnecessary columns == #\n",
    "    # i. Remove counter-type features (rainy_days and dry_days)\n",
    "    drop_cols = [c for c in feats.columns if ('rainy_days' in c or 'dry_days' in c)]\n",
    "    if drop_cols:\n",
    "        feats = feats.drop(columns=drop_cols)\n",
    "\n",
    "    # ii. NaNs in saturation indices => 0 (neutral)\n",
    "    sat_cols = [c for c in feats.columns if 'sat' in c]\n",
    "    if sat_cols:\n",
    "        feats[sat_cols] = feats[sat_cols].fillna(0)\n",
    "\n",
    "    # iii. NaNs in event_strength => 0 (no event = neutral)\n",
    "    evt_cols = [c for c in feats.columns if 'event_strength' in c]\n",
    "    if evt_cols:\n",
    "        feats[evt_cols] = feats[evt_cols].fillna(0)\n",
    "\n",
    "    # iv. Remove raw meteorological columns\n",
    "    feats = feats.drop(columns=meteo_col_list)\n",
    "    R_ER_features_df = feats.copy()\n",
    "\n",
    "    return R_ER_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256463b2-ab39-4e7d-8594-f407847ac65f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Helper functions to compute groundwater level (GWL) features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6f4323-8ccf-42d7-bc84-4c7401458393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Helper: Lagged series ===== #\n",
    "def _lag_blocks(s, shift):\n",
    "    \n",
    "    \"\"\"Lagged series (shifted by 'shift' steps).\"\"\"\n",
    "    return s.shift(shift)\n",
    "\n",
    "\n",
    "# ===== Helper: Differences and rolling means ===== #\n",
    "def _diff_mean_pipeline(s, lag_name, periods):\n",
    "\n",
    "    \"\"\"Computes differences and rolling means over multiple periods P.\"\"\"\n",
    "    out = {}\n",
    "    for P in periods:\n",
    "        out[f'{lag_name}_diff{P}'] = s.diff(P)\n",
    "        out[f'{lag_name}_mean{P}'] = s.rolling(window=P).mean()\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ===== Helper: Rolling extrema ===== #\n",
    "def _extrema_pipeline(s, lag_name, windows):\n",
    "\n",
    "    \"\"\"Computes rolling max/min over multiple window sizes.\"\"\"\n",
    "    out = {}\n",
    "    for w in windows:\n",
    "        out[f'{lag_name}_max{w}'] = s.rolling(window=w, min_periods=1).max()\n",
    "        out[f'{lag_name}_min{w}'] = s.rolling(window=w, min_periods=1).min()\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ===== Helper: Deviation from global mean ===== #\n",
    "def _global_mean_diff(s, lag_name, global_mean):\n",
    "\n",
    "    \"\"\"Deviation from the (constant) global mean.\"\"\"\n",
    "    out = {}\n",
    "    out[f'{lag_name}_diff_Gmean'] = s - global_mean\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aac1814-f643-422e-b8ad-1d62bec843cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Function to compute GWL features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaf5bda-43cf-4b4a-b14b-0f428534a0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Compute GWL features =====\n",
    "def gwl_features_calcul(df, hydro_col_list, shift_list=(0, 1, 5, 8, 20, 30), \n",
    "                        P_periods=(2, 5, 10, 20, 30, 60), W_periods=(5, 10, 20, 30, 60, 90)):\n",
    "\n",
    "    base_df = [df[['Date']].copy()]\n",
    "\n",
    "    for GWL_col in hydro_col_list:\n",
    "        s = df[GWL_col]\n",
    "        gmean = s.mean()\n",
    "\n",
    "        feat_dict = {}\n",
    "        for sh in shift_list:\n",
    "            lag_s = _lag_blocks(s, sh)\n",
    "            lag_name = f'{GWL_col}_lag{sh}'\n",
    "\n",
    "            # The lagged series \n",
    "            feat_dict[lag_name] = lag_s\n",
    "\n",
    "            # Feature blocks\n",
    "            feat_dict.update(_diff_mean_pipeline(lag_s, lag_name, P_periods))\n",
    "            feat_dict.update(_extrema_pipeline(lag_s, lag_name, W_periods))\n",
    "            feat_dict.update(_global_mean_diff(lag_s, lag_name, gmean))\n",
    "\n",
    "        # Concatenate\n",
    "        base_df.append(pd.DataFrame(feat_dict, index=df.index))\n",
    "\n",
    "    GWL_features_df = pd.concat(base_df, axis=1)\n",
    "\n",
    "    return GWL_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f28633-e58a-45cc-b56d-d174826a692e",
   "metadata": {},
   "source": [
    "### Helper functions for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9197e3e0-c269-485f-a956-0a78c8b8b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Compute features over the full time series ===== #\n",
    "def compute_features(hm_df, externe_col_list, meteo_col_list=('R', 'ER'), hydro_col_list=('GWL',)):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes features over the full time series (including the forecast period).\n",
    "\n",
    "    The purpose is to compute these features only once, and then reuse the features derived from\n",
    "    the true observed values within the XGBoost workflow.\n",
    "    \"\"\"\n",
    "    \n",
    "    if hm_df.empty:\n",
    "        features_hm_df = pd.DataFrame({\"Date\": []})\n",
    "    else:\n",
    "        R_ER_features_df = rainfall_and_effective_rainfall_features_calcul(hm_df, list(meteo_col_list))\n",
    "        GWL_features_df = gwl_features_calcul(hm_df, list(hydro_col_list))\n",
    "        features_hm_df = reduce(lambda L, R: pd.merge(L, R, on='Date', how='outer'), [R_ER_features_df, GWL_features_df])\n",
    "\n",
    "    if externe_col_list:\n",
    "        extern_part = hm_df[['Date'] + [c for c in externe_col_list if c in hm_df.columns]].copy()\n",
    "        R_ER_features_df = R_ER_features_df.merge(extern_part, on='Date', how='left')\n",
    "        GWL_features_df = GWL_features_df.merge(extern_part, on='Date', how='left')\n",
    "        features_hm_df = features_hm_df.merge(extern_part, on='Date', how='left')\n",
    "\n",
    "    return features_hm_df, R_ER_features_df, GWL_features_df\n",
    "\n",
    "\n",
    "# ===== Compute features over the full time series (Séchilienne) ===== #\n",
    "def compute_features_Sech(hm_df, externe_col_list, meteo_col_list=('R', 'ER'), hydro_col_list=('WLI', 'WLM')):\n",
    "\n",
    "    \"\"\"\n",
    "    Same as compute_features, but adapted to Séchilienne and its two water-level columns.\n",
    "    \"\"\"\n",
    "    if hm_df.empty:\n",
    "        features_hm_df = pd.DataFrame({\"Date\": []})\n",
    "    else:\n",
    "        R_ER_features_df = rainfall_and_effective_rainfall_features_calcul(hm_df, list(meteo_col_list))\n",
    "        GWL_features_df = gwl_features_calcul(hm_df, list(hydro_col_list))\n",
    "        features_hm_df = reduce(lambda L, R: pd.merge(L, R, on='Date', how='outer'), [R_ER_features_df, GWL_features_df])\n",
    "\n",
    "    if externe_col_list:\n",
    "        extern_part = hm_df[['Date'] + [c for c in externe_col_list if c in hm_df.columns]].copy()\n",
    "        R_ER_features_df = R_ER_features_df.merge(extern_part, on='Date', how='left')\n",
    "        GWL_features_df = GWL_features_df.merge(extern_part, on='Date', how='left')\n",
    "        features_hm_df = features_hm_df.merge(extern_part, on='Date', how='left')\n",
    "\n",
    "    return features_hm_df, R_ER_features_df, GWL_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e352a1ad-2ddf-4e7e-aeb4-42b9485475bd",
   "metadata": {},
   "source": [
    "### Saving functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1028b2-7fc2-4625-85f2-4b818f83210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Save the file containing the features ===== #\n",
    "def save_features_df(features_hm_df, R_ER_features_df, GWL_features_df, output_folder_path):\n",
    "    \n",
    "    out_dir = os.path.join(output_folder_path, '1_Features_ER_R_GWL')\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    features_hm_df.to_excel(os.path.join(out_dir, 'Features_HM.xlsx'), index=False)\n",
    "    R_ER_features_df.to_excel(os.path.join(out_dir, 'Features_METEO.xlsx'), index=False)\n",
    "    GWL_features_df.to_excel(os.path.join(out_dir, 'Features_HYDRO.xlsx'), index=False)\n",
    "\n",
    "    print(f\"Files successfully saved in {out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0debc1-2cb7-468d-9f4e-fdf2791b9202",
   "metadata": {},
   "source": [
    "## End-to-end pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9eaa59-976f-4ab1-9703-4ed2e4a10009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_pipeline_totale(input_path, input_file, input_usecols, externe_col_list, output_folder_path):\n",
    "    \"\"\"Viella and Villerville version\"\"\"\n",
    "\n",
    "    # ===== 1. Data import ===== #\n",
    "    hm_df = data_importation(input_path, input_file, input_usecols)\n",
    "\n",
    "    # ===== 2. Feature computation ===== #\n",
    "    features_hm_df, R_ER_features_df, GWL_features_df = compute_features(hm_df, externe_col_list, meteo_col_list=('R', 'ER'), \n",
    "                                                                         hydro_col_list=('GWL',))\n",
    "\n",
    "    # ===== 3. Saving ===== #\n",
    "    save_features_df(features_hm_df, R_ER_features_df, GWL_features_df, output_folder_path)\n",
    "\n",
    "    return features_hm_df, R_ER_features_df, GWL_features_df\n",
    "\n",
    "\n",
    "def features_pipeline_totale_Sech(input_path, input_file, input_usecols, externe_col_list, output_folder_path):\n",
    "    \"\"\"Version Séchilienne\"\"\"\n",
    "\n",
    "    # ===== 1. Data import ===== #\n",
    "    hm_df = data_importation_Sech(input_path, input_file, input_usecols)\n",
    "\n",
    "    # ===== 2. Features computation ===== #\n",
    "    features_hm_df, R_ER_features_df, GWL_features_df = compute_features(hm_df, externe_col_list, meteo_col_list=('R', 'ER'), \n",
    "                                                                         hydro_col_list=('WLI', 'WLM'))\n",
    "\n",
    "    # ===== 3. Saving ===== #\n",
    "    save_features_df(features_hm_df, R_ER_features_df, GWL_features_df, output_folder_path)\n",
    "\n",
    "    return features_hm_df, R_ER_features_df, GWL_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e129a7cf-3ef3-4c1c-b0bc-221398a8cbb6",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae15aea4-bb89-4d53-8279-a29933ee51ba",
   "metadata": {},
   "source": [
    "### Viella and Villerville"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b86e91-b62e-4e77-a4de-ea89ca24ac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 1. Parameters ===== #\nfrom pathlib import Path\n\nSITE = \"Villerville\"\n\n# Repo-friendly paths (relative to the project root)\nPROJECT_ROOT = Path.cwd()  # If running from /notebooks, you may want Path.cwd().parent\ninput_path = PROJECT_ROOT / \"data\" / SITE / \"0_Input_dataset\"\ninput_file = \"Hydro_Meteo_traitées.xlsx\"  # Adapt to the site / dataset\ninput_usecols = ['R', 'ER', 'PZ3']  # Columns to read from the Excel file (excluding 'Date')\nexterne_col_list = ['R', 'ER', 'GWL']  # External series to keep in the output\n\noutput_folder_path = PROJECT_ROOT / \"outputs\" / SITE\n\n# ===== 2. Run ===== #\nfeatures_hm_df, R_ER_features_df, GWL_features_df = features_pipeline_totale(\n    input_path, input_file, input_usecols,\n    externe_col_list, output_folder_path\n)\n\nfeatures_hm_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99911718-af91-4889-97a6-ace254b6ded8",
   "metadata": {},
   "source": [
    "### Séchilienne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf988a3-ae8c-445f-9975-fe07153a36b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 1. Parameters ===== #\nfrom pathlib import Path\n\nSITE = \"Séchilienne\"\n\n# Repo-friendly paths (relative to the project root)\nPROJECT_ROOT = Path.cwd()  # If running from /notebooks, you may want Path.cwd().parent\ninput_path = PROJECT_ROOT / \"data\" / SITE / \"0_Input_dataset\"\ninput_file = \"Hydro_Meteo_traitées.xlsx\"  # Adapt to the site / dataset\ninput_usecols = ['R', 'ER', 'WLI', 'WLM']  # Columns to read from the Excel file (excluding 'Date')\nexterne_col_list = ['R', 'ER', 'WLI', 'WLM']  # External series to keep in the output\n\noutput_folder_path = PROJECT_ROOT / \"outputs\" / SITE\n\n# ===== 2. Run ===== #\nfeatures_hm_df, R_ER_features_df, GWL_features_df = features_pipeline_totale_Sech(\n    input_path, input_file, input_usecols,\n    externe_col_list, output_folder_path\n)\n\nfeatures_hm_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GWL-LSTM)",
   "language": "python",
   "name": "gwl-lstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}